{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+MQFuw7Pk8059wdyTLcH4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","import random\n","custom_map = [\n","    \"SFFF\",\n","    \"FHFH\",\n","    \"FFFH\",\n","    \"HFFG\"\n","]\n","env = gym.make(\"FrozenLake-v1\",desc=custom_map, is_slippery=True)\n","\n","state_space_size = env.observation_space.n\n","action_space_size = env.action_space.n\n","\n","print(\"State space:\", state_space_size)\n","print(\"Action space:\", action_space_size)\n","q_table = np.zeros((state_space_size, action_space_size))\n","episodes = 30000\n","max_steps_per_episode = 2000\n","\n","learning_rate = 0.8\n","discount_factor = 0.95\n","\n","epsilon_start = 1.0\n","epsilon_end = 0.001\n","epsilon_decay = 0.999\n","rewards_all_episodes = []\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1E9s5K2JkPL6","executionInfo":{"status":"ok","timestamp":1765981468200,"user_tz":-330,"elapsed":11,"user":{"displayName":"Uday adithya","userId":"02386049917281247131"}},"outputId":"41e9d165-1343-44de-a361-0a0639af093a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["State space: 16\n","Action space: 4\n"]}]},{"cell_type":"code","source":["for episode in range(episodes):\n","    state, _ = env.reset()\n","    total_rewards = 0\n","\n","    epsilon = max(\n","        epsilon_end,\n","        epsilon_start * (epsilon_decay ** episode)\n","    )\n","\n","    for step in range(max_steps_per_episode):\n","\n","\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            action = np.argmax(q_table[state, :])\n","\n","        new_state, reward, terminated, truncated, _ = env.step(action)\n","\n","\n","        q_table[state, action] = q_table[state, action] + learning_rate * (\n","            reward\n","            + discount_factor * np.max(q_table[new_state, :])\n","            - q_table[state, action]\n","        )\n","\n","        state = new_state\n","        total_rewards += reward\n","\n","\n","        if terminated or truncated:\n","            break\n","\n","    rewards_all_episodes.append(total_rewards)\n"],"metadata":{"id":"0W_tngHvl0nS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training**"],"metadata":{"id":"98DfI5ZlmCm2"}},{"cell_type":"code","source":["print(\"Average reward per 1000 episodes:\")\n","for i in range(0, episodes, 1000):\n","    avg_reward = np.mean(rewards_all_episodes[i:i+1000])\n","    print(f\"{i} - {i+1000}: {avg_reward}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-e3q1UomFI4","executionInfo":{"status":"ok","timestamp":1765981504401,"user_tz":-330,"elapsed":9,"user":{"displayName":"Uday adithya","userId":"02386049917281247131"}},"outputId":"66d813d7-bb28-43d2-ab58-e58f6730feaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average reward per 1000 episodes:\n","0 - 1000: 0.028\n","1000 - 2000: 0.088\n","2000 - 3000: 0.186\n","3000 - 4000: 0.38\n","4000 - 5000: 0.459\n","5000 - 6000: 0.534\n","6000 - 7000: 0.681\n","7000 - 8000: 0.7\n","8000 - 9000: 0.645\n","9000 - 10000: 0.694\n","10000 - 11000: 0.667\n","11000 - 12000: 0.703\n","12000 - 13000: 0.664\n","13000 - 14000: 0.661\n","14000 - 15000: 0.68\n","15000 - 16000: 0.698\n","16000 - 17000: 0.67\n","17000 - 18000: 0.691\n","18000 - 19000: 0.691\n","19000 - 20000: 0.707\n","20000 - 21000: 0.676\n","21000 - 22000: 0.643\n","22000 - 23000: 0.687\n","23000 - 24000: 0.697\n","24000 - 25000: 0.683\n","25000 - 26000: 0.616\n","26000 - 27000: 0.637\n","27000 - 28000: 0.655\n","28000 - 29000: 0.621\n","29000 - 30000: 0.691\n"]}]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"gq8qqj5-mcZ4"}},{"cell_type":"code","source":["test_episodes = 100\n","success_count = 0\n","\n","for episode in range(test_episodes):\n","    state, _ = env.reset()\n","\n","    for step in range(max_steps_per_episode):\n","\n","\n","        action = np.argmax(q_table[state, :])\n","\n","        new_state, reward, terminated, truncated, _ = env.step(action)\n","\n","        state = new_state\n","\n","\n","        if terminated or truncated:\n","            if reward == 1:\n","                success_count += 1\n","            break\n","\n","print(f\"Success rate over {test_episodes} episodes: {success_count}%\")\n","\n","print(\"Optimal policy (action per state):\")\n","print(np.argmax(q_table, axis=1))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHUUa0RImkZV","executionInfo":{"status":"ok","timestamp":1765982267563,"user_tz":-330,"elapsed":65,"user":{"displayName":"Uday adithya","userId":"02386049917281247131"}},"outputId":"a3baa824-9aef-4caa-e70c-d07ee9ead848"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Success rate over 100 episodes: 74%\n","Optimal policy (action per state):\n","[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n"]}]}]}