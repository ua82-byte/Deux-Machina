{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee456cb6-16a3-431d-84b6-cfcac84ecb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 100 episodes\n",
      "Reward_avg for last 100= 87.0\n",
      "Finished 200 episodes\n",
      "Reward_avg for last 100= 87.0\n",
      "Finished 300 episodes\n",
      "Reward_avg for last 100= 87.0\n",
      "Finished 400 episodes\n",
      "Reward_avg for last 100= 87.0\n",
      "Finished 500 episodes\n",
      "Reward_avg for last 100= 87.0\n",
      "Finished 600 episodes\n",
      "Reward_avg for last 100= 87.0\n",
      "Finished 700 episodes\n",
      "Reward_avg for last 100= 87.0\n",
      "Finished 800 episodes\n",
      "Reward_avg for last 100= 87.0\n",
      "Finished 900 episodes\n",
      "Reward_avg for last 100= 87.0\n",
      "Finished 1000 episodes\n",
      "Reward_avg for last 100= 87.0\n"
     ]
    }
   ],
   "source": [
    "# cell for training\n",
    "#Code was made by me using the lunar landing and k-armed bandit problem as template\n",
    "# You can change training map and choose desired one in the code below,currently it is set to random map generation\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "class Agent:\n",
    "    def __init__(self,expl,lr,gamma,size):\n",
    "        self.Q=np.zeros((size*size,4))\n",
    "        self.V=np.zeros(size*size)\n",
    "        self.expl=expl\n",
    "        self.initialexpl=expl\n",
    "        self.gamma=gamma\n",
    "        self.lr=lr\n",
    "        self.size=size\n",
    "    def upd_expl(self,no_episodes):\n",
    "        self.expl-=9/(10*no_episodes)*self.initialexpl #decreasing tendency to explore as time progresses,to use more exploitation\n",
    "    def upd_Q(self,prev_state,next_state,action,reward):\n",
    "        self.Q[prev_state,action] = self.Q[prev_state,action]*(1-self.lr) + self.lr*(reward+self.gamma*np.max(self.Q[next_state]))\n",
    "    def get_action(self,state):\n",
    "        if np.random.random() < self.expl: # explore\n",
    "            return np.random.randint(4)\n",
    "        else: # exploit\n",
    "            return np.random.choice(np.flatnonzero(self.Q[state] == self.Q[state].max()))\n",
    "    def get_action1(self,state):#action used by trained agent doesnt require exploration\n",
    "        return np.random.choice(np.flatnonzero(self.Q[state] == self.Q[state].max()))\n",
    "\n",
    "\n",
    "def experiment(no_episodes,descr,mp_name,size):\n",
    "    env = gym.make(\n",
    "        'FrozenLake-v1',\n",
    "        desc=descr,\n",
    "        map_name=mp_name,\n",
    "        is_slippery=False,\n",
    "        reward_schedule=(reward_goal,reward_hole ,reward_move)\n",
    "        \n",
    "    )\n",
    "    agent=Agent(expl,lr,gamma,size)\n",
    "    for episode in range(no_episodes):\n",
    "    # Get the first observation -> Initial State\n",
    "        state1, info = env.reset()\n",
    "        reward_10list=[]\n",
    "        total_reward = 0\n",
    "        run =True\n",
    "        while (run):\n",
    "            action=agent.get_action(state1)\n",
    "            state2, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            agent.upd_Q(state1,state2,action,reward)\n",
    "            state1=state2\n",
    "            # If the episode has ended then we can reset to start a new episode\n",
    "            if terminated or truncated:\n",
    "                run = False\n",
    "        reward_10list.append(total_reward)     \n",
    "        total_reward=0\n",
    "        agent.upd_expl(no_episodes)#decreasing tendency to explore as more episodes are completed,to use more exploitation\n",
    "        #For checking progress intermitedly\n",
    "        if((episode+1)%100==0): \n",
    "            print(\"Finished\",episode+1,\"episodes\")\n",
    "            print(\"Reward_avg for last 100=\",np.sum(reward_10list)/len(reward_10list))\n",
    "            reward_10list=[]\n",
    "    env.close()\n",
    "    return agent #Trained agent returned\n",
    "\n",
    "# Rewards set up to minimize unnecessary movement and termination due to falling in holes\n",
    "reward_move=-1\n",
    "reward_hole=-30\n",
    "reward_goal=100\n",
    "expl=0.1\n",
    "lr=0.8\n",
    "gamma=1  # as total reward sums reward of all steps,and goal is to achieve the total reward so i believe discount factor here is not needed,so it is set to 1\n",
    "no_episodes_to_train=1000 # can be increased for more fine tuning but seems to be sufficient\n",
    "mp_name=None\n",
    "descr=generate_random_map(size=8)# map can be changed to any map by manually creating or using mp_name\n",
    "size=len(descr) # if we use a 4x4 map size is said to be 4\n",
    "agent=experiment(no_episodes_to_train,descr,mp_name,size)#agent is returned from function after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward= 87\n",
      "Goal Reached\n"
     ]
    }
   ],
   "source": [
    "#Cell for running final trained agent\n",
    "env = gym.make(\n",
    "      'FrozenLake-v1',\n",
    "       desc=descr,\n",
    "       map_name=mp_name,\n",
    "       is_slippery=False,\n",
    "       reward_schedule=(reward_goal,reward_hole ,reward_move),\n",
    "       render_mode=\"human\" \n",
    "    )\n",
    "run=True\n",
    "state1, info = env.reset()\n",
    "total_reward = 0\n",
    "while (run):\n",
    "    action=agent.get_action1(state1)#getaction1 doesnt involve exploration\n",
    "    state1, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        run = False\n",
    "print(\"Total reward=\",total_reward)\n",
    "if (total_reward==(reward_goal-2*size+3)):\n",
    "    print(\"Goal Reached\")\n",
    "else :\n",
    "    print(\"Mission failed....\")\n",
    "env.close()\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
