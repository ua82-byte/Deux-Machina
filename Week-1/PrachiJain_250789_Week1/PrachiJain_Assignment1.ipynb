{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Initialise the environment\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "env = gym.make(\n",
    "    'FrozenLake-v1',\n",
    "    desc=generate_random_map(size=4),    # generating a random map\n",
    "    is_slippery=True,                    # creating a slippery environment i.e. stochastic environment\n",
    "    success_rate=1.0/3.0,\n",
    "    reward_schedule=(1, -1, 0)           # reward: 1 for goal, -1 for hole, 0 for frozen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe693ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "l_rate = 0.8            # learning rate\n",
    "gamma = 0.95            # discount factor\n",
    "epsilon = 1.0           # epsilon\n",
    "min_epsilon = 0.01      # minimum value of epsilon\n",
    "decay_ratio = 0.001     # decay ratio for decaying epsilon\n",
    "num_episodes = 2000     # total no. of episodes for training\n",
    "n_states = env.observation_space.n         # no. of states\n",
    "n_action = env.action_space.n              # no. of actions possible\n",
    "# A Q-table containing Q(S,A) pairs defining estimated optimal policy Ï€*\n",
    "q_table = np.zeros((n_states, n_action))\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    # Initialize state S\n",
    "    state, info = env.reset()\n",
    "    run = True\n",
    "    while(run):\n",
    "        # Choose A from S using epsilon-greedy algorithm\n",
    "        n = np.random.rand(1)\n",
    "        if n < epsilon:  # Exploration\n",
    "            # randomly decide on an action\n",
    "            action = np.random.choice(range(n_action))\n",
    "        else:       # Exploitaion\n",
    "            # choose policy using Q-Table\n",
    "            # choosing the max Q(s, a) value for given state; in case of multiple occurreces of the value, choosing randomly among them\n",
    "            action = np.random.choice((list(np.where(q_table[state, :]==np.max(q_table[state, :]))))[0])\n",
    "            \n",
    "        # making observation after taking action\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        q_observed = reward + gamma * (np.max(q_table[next_state, :]))     # observed Q(s, a)\n",
    "        q_expected = q_table[state, action]          # expected Q(s, a) from Q-Table\n",
    "        tde = q_observed - q_expected             # calculating Temporal Difference Error\n",
    "        \n",
    "        # updating Q-Table\n",
    "        q_table[state, action] += l_rate * tde\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if(terminated or truncated):\n",
    "            run = False\n",
    "    \n",
    "    # updating the value of epsilon (by simplified exponential decay)\n",
    "    epsilon = max(min_epsilon, epsilon*decay_ratio)       # min_epsilon ensures some minimum exploration at all times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b9228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Q-Table\n",
    "np.save('q_table.npy', q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ce3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "num_episodes = 100\n",
    "n_holes = 0\n",
    "n_goals = 0\n",
    "total_reward = 0\n",
    "for ep in range(num_episodes):\n",
    "    # Initialize state S\n",
    "    state, info = env.reset()\n",
    "    run = True\n",
    "    while(run):\n",
    "        # Choose A from S using optimal policy from Q-Table\n",
    "        # choosing the max Q(s, a) value for given state; in case of multiple occurreces of the value, choosing randomly among them\n",
    "        action = np.random.choice((list(np.where(q_table[state, :]==np.max(q_table[state, :]))))[0])\n",
    "        \n",
    "        # making observation after taking action\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # storing reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if(terminated or truncated):\n",
    "            if reward==1:\n",
    "                n_goals += 1\n",
    "            elif reward==-1:\n",
    "                n_holes += 1\n",
    "            run = False\n",
    "            \n",
    "avg_reward = total_reward / num_episodes       # calculating avg reward\n",
    "success_rate = n_goals/num_episodes*100        # calculating success rate\n",
    "failure_rate = n_holes/num_episodes*100        # calculating failure rate\n",
    "# Printing Test Results\n",
    "print(f'Success Rate = {success_rate}')\n",
    "print(f'Failure Rate = {failure_rate}')\n",
    "print(f'Average Reward = {avg_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
